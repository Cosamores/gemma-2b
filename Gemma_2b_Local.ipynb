{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Implementação de um modelo de LLM para uso local**\n",
        "\n",
        "- Você pode encontrar diversos modelos em:\n",
        "  -  https://huggingface.co/models\n",
        "  - https://www.kaggle.com/models/\n",
        "\n",
        "\n",
        "- Nesta implementação utilizaremos o modelo Gemma-2b Instruct do Google baixado do site da Hugging Face. Você pode encontrar este modelo aqui: [google/gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n",
        "\n",
        "- Atente-se para o tempo de download do modelo e à característica stateless do colab, pois aqui os dados importados desaparecem e todos os processos devem ser reiniciados após o fim da sessão.\n",
        "\n",
        "- Obtenha um token de acesso da Hugging Face através da página https://huggingface.co/settings/tokens, alguns modelos dependem deste token para serem acessados.\n",
        "\n",
        "- Esta é uma implementação básica de um modelo pré-treinado. Com algumas alterações no código podemos combinar nossos próprios documentos para obter respostas mais específicas. Estas alterações variaamde acordo com a necessidade do projeto e podem ser tratadas em outros notebooks\n",
        "\n",
        "\n",
        "*Criado por: [Diego Cosamores](https://cosamores.com)*"
      ],
      "metadata": {
        "id": "N6qqDHHr0ze7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow5gRayOw_h8"
      },
      "outputs": [],
      "source": [
        "# Login na Hugging Face (se for usar modelos da HF)\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Defina o token como variável de ambiente (recomendado) ou adicione-o abaixo substituindo a string \"TOKEN\" (não recomendado)\n",
        "token = os.getenv(\"TOKEN\")\n",
        "\n",
        "# Login na HF\n",
        "login(token)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Aqui você define o modelo que vai usar de acordo com o link do modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", torch_dtype=torch.bfloat16)\n"
      ],
      "metadata": {
        "id": "rdIk_v75xPNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui você define o número máximo de caracteres no texto de entrada\n",
        "max_length = 1024\n",
        "\n",
        "# Texto de entrada / prompt\n",
        "input_text = \"Write a poem about yourself as a language model, and how you feel about it, if you are able to feel.\"\n",
        "\n",
        "# Transformação do texto em inputs para o modelo no formato de tensores\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = torch.tensor(input_ids['input_ids']).clone().detach().unsqueeze(0)\n",
        "input_ids = input_ids[0]\n",
        "\n",
        "# Imprime o array de tensores formado\n",
        "print(input_ids)\n",
        "\n",
        "# imprime o número de dimensões da entrada\n",
        "print(input_ids.shape)"
      ],
      "metadata": {
        "id": "eI4VPcejxnb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar o output\n",
        "outputs = model.generate(input_ids, max_length=max_length)"
      ],
      "metadata": {
        "id": "7euolbKyyeCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decodificar a saída gerada\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Imprimir a saída gerada (poema)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "qdVfECKlyh3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}